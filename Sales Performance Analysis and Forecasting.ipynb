{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b169e044-8f59-4935-86e7-294cf79a7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import zscore\n",
    "import datetime as dt\n",
    "from sklearn.model_selection import train_test_split,learning_curve,GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import joblib\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59afbc7e-3191-42ca-9dd0-d0bb339cfa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Row ID        Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
      "0       1  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "1       2  CA-2017-152156  08/11/2017  11/11/2017    Second Class    CG-12520   \n",
      "2       3  CA-2017-138688  12/06/2017  16/06/2017    Second Class    DV-13045   \n",
      "3       4  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "4       5  US-2016-108966  11/10/2016  18/10/2016  Standard Class    SO-20335   \n",
      "\n",
      "     Customer Name    Segment        Country             City       State  \\\n",
      "0      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "1      Claire Gute   Consumer  United States        Henderson    Kentucky   \n",
      "2  Darrin Van Huff  Corporate  United States      Los Angeles  California   \n",
      "3   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "4   Sean O'Donnell   Consumer  United States  Fort Lauderdale     Florida   \n",
      "\n",
      "   Postal Code Region       Product ID         Category Sub-Category  \\\n",
      "0      42420.0  South  FUR-BO-10001798        Furniture    Bookcases   \n",
      "1      42420.0  South  FUR-CH-10000454        Furniture       Chairs   \n",
      "2      90036.0   West  OFF-LA-10000240  Office Supplies       Labels   \n",
      "3      33311.0  South  FUR-TA-10000577        Furniture       Tables   \n",
      "4      33311.0  South  OFF-ST-10000760  Office Supplies      Storage   \n",
      "\n",
      "                                        Product Name     Sales  \n",
      "0                  Bush Somerset Collection Bookcase  261.9600  \n",
      "1  Hon Deluxe Fabric Upholstered Stacking Chairs,...  731.9400  \n",
      "2  Self-Adhesive Address Labels for Typewriters b...   14.6200  \n",
      "3      Bretford CR4500 Series Slim Rectangular Table  957.5775  \n",
      "4                     Eldon Fold 'N Roll Cart System   22.3680  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Row ID         9800 non-null   int64  \n",
      " 1   Order ID       9800 non-null   object \n",
      " 2   Order Date     9800 non-null   object \n",
      " 3   Ship Date      9800 non-null   object \n",
      " 4   Ship Mode      9800 non-null   object \n",
      " 5   Customer ID    9800 non-null   object \n",
      " 6   Customer Name  9800 non-null   object \n",
      " 7   Segment        9800 non-null   object \n",
      " 8   Country        9800 non-null   object \n",
      " 9   City           9800 non-null   object \n",
      " 10  State          9800 non-null   object \n",
      " 11  Postal Code    9789 non-null   float64\n",
      " 12  Region         9800 non-null   object \n",
      " 13  Product ID     9800 non-null   object \n",
      " 14  Category       9800 non-null   object \n",
      " 15  Sub-Category   9800 non-null   object \n",
      " 16  Product Name   9800 non-null   object \n",
      " 17  Sales          9800 non-null   float64\n",
      "dtypes: float64(2), int64(1), object(15)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "(9800, 18)\n",
      "             Row ID        Order ID  Order Date   Ship Date       Ship Mode  \\\n",
      "count   9800.000000            9800        9800        9800            9800   \n",
      "unique          NaN            4922        1230        1326               4   \n",
      "top             NaN  CA-2018-100111  05/09/2017  26/09/2018  Standard Class   \n",
      "freq            NaN              14          38          34            5859   \n",
      "mean    4900.500000             NaN         NaN         NaN             NaN   \n",
      "std     2829.160653             NaN         NaN         NaN             NaN   \n",
      "min        1.000000             NaN         NaN         NaN             NaN   \n",
      "25%     2450.750000             NaN         NaN         NaN             NaN   \n",
      "50%     4900.500000             NaN         NaN         NaN             NaN   \n",
      "75%     7350.250000             NaN         NaN         NaN             NaN   \n",
      "max     9800.000000             NaN         NaN         NaN             NaN   \n",
      "\n",
      "       Customer ID  Customer Name   Segment        Country           City  \\\n",
      "count         9800           9800      9800           9800           9800   \n",
      "unique         793            793         3              1            529   \n",
      "top       WB-21850  William Brown  Consumer  United States  New York City   \n",
      "freq            35             35      5101           9800            891   \n",
      "mean           NaN            NaN       NaN            NaN            NaN   \n",
      "std            NaN            NaN       NaN            NaN            NaN   \n",
      "min            NaN            NaN       NaN            NaN            NaN   \n",
      "25%            NaN            NaN       NaN            NaN            NaN   \n",
      "50%            NaN            NaN       NaN            NaN            NaN   \n",
      "75%            NaN            NaN       NaN            NaN            NaN   \n",
      "max            NaN            NaN       NaN            NaN            NaN   \n",
      "\n",
      "             State   Postal Code Region       Product ID         Category  \\\n",
      "count         9800   9789.000000   9800             9800             9800   \n",
      "unique          49           NaN      4             1861                3   \n",
      "top     California           NaN   West  OFF-PA-10001970  Office Supplies   \n",
      "freq          1946           NaN   3140               19             5909   \n",
      "mean           NaN  55273.322403    NaN              NaN              NaN   \n",
      "std            NaN  32041.223413    NaN              NaN              NaN   \n",
      "min            NaN   1040.000000    NaN              NaN              NaN   \n",
      "25%            NaN  23223.000000    NaN              NaN              NaN   \n",
      "50%            NaN  58103.000000    NaN              NaN              NaN   \n",
      "75%            NaN  90008.000000    NaN              NaN              NaN   \n",
      "max            NaN  99301.000000    NaN              NaN              NaN   \n",
      "\n",
      "       Sub-Category     Product Name         Sales  \n",
      "count          9800             9800   9800.000000  \n",
      "unique           17             1849           NaN  \n",
      "top         Binders  Staple envelope           NaN  \n",
      "freq           1492               47           NaN  \n",
      "mean            NaN              NaN    230.769059  \n",
      "std             NaN              NaN    626.651875  \n",
      "min             NaN              NaN      0.444000  \n",
      "25%             NaN              NaN     17.248000  \n",
      "50%             NaN              NaN     54.490000  \n",
      "75%             NaN              NaN    210.605000  \n",
      "max             NaN              NaN  22638.480000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/user/Desktop/Superstore sales.csv\")\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Get information about the dataset\n",
    "print(df.info())\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# Summary statistics of the dataset\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df4ca9-58db-4388-8ed4-e53f7118bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f8476-2713-4b03-9573-1dff658f8132",
   "metadata": {},
   "source": [
    "## Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dab81b-85b0-49c0-9835-84afe52e82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualizing missing values\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Drop or impute missing values if necessary\n",
    "# Example: Dropping rows with missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade4ac-f127-4758-a2e0-a51a59b8ca54",
   "metadata": {},
   "source": [
    "## Data Types and Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5ebb7-3546-414d-999c-c3999c5151d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' and 'Ship Date' to datetime\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], dayfirst = True)\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], dayfirst = True)\n",
    "\n",
    "# Check data types again after conversion\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad1e91b-2cad-44aa-8356-52722464b4e8",
   "metadata": {},
   "source": [
    " ## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505f5a2-33fe-49c3-8e97-c0123593106c",
   "metadata": {},
   "source": [
    "### Conceptual Overview of Exploratory Data Analysis (EDA)\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in understanding and interpreting the data we collect. Here’s a brief overview of its significance:\n",
    "\n",
    "#### 1. Purpose of EDA\n",
    "EDA involves examining the data to uncover patterns, spot anomalies, test hypotheses, and check assumptions with the help of summary statistics and graphical representations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Key Objectives\n",
    "- **Data Quality Assessment**: Identify and address missing values, outliers, and inconsistencies.\n",
    "- **Pattern Detection**: Discover relationships and trends within the data that can inform business strategies.\n",
    "- **Hypothesis Generation**: Develop new hypotheses and insights based on initial findings from the data.\n",
    "\n",
    "---\n",
    "#### 3. Techniques Used\n",
    "- **Descriptive Statistics**: Summarize data using measures such as mean, median, and standard deviation.\n",
    "- **Visualization**: Utilize charts and graphs like histograms, box plots, and scatter plots to visually explore data.\n",
    "- **Correlation Analysis**: Assess the relationships between different variables to identify potential drivers of observed patterns.\n",
    "---\n",
    "\n",
    "\n",
    "#### 4. Business Implications\n",
    "EDA provides a solid foundation for data-driven decision-making by:\n",
    "- Enhancing our understanding of underlying data structures.\n",
    "- Informing the design of further analyses and modeling approaches.\n",
    "- Highlighting key areas of focus for improving business performance.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Through EDA, we gain critical insights that help us make more informed and effective business decisions, ultimately driving better outcomes and achieving our strategic goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299692bb-5b2a-461e-9e25-2c550b503712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70d6e0-d4ad-4c1b-9ec8-6ff20f1f359c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645d6dd-a283-4e78-9720-d1e92a6ee00b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e3cbae5-2657-43c7-a02b-95c12de46bdf",
   "metadata": {},
   "source": [
    " ### Univariate Analysis\n",
    " #### Conceptual Overview of Univariate Analysis\n",
    " Univariate analysis is a fundamental statistical method that examines a single variable in isolation to understand its characteristics, distribution, and behavior. It is often the first step in data analysis, providing insights into the data structure and helping to identify trends, patterns, or anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e464f-af0a-48a7-ae15-d09b4a69f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of Sales\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"Sales\"], kde=True)\n",
    "plt.title(\"Distribution of Sales\")\n",
    "plt.xlabel(\"Sales\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Analyzing categorical variables (e.g., Segment, Category, Ship Mode)\n",
    "categorical_cols = [\"Segment\", \"Category\", \"Ship Mode\", \"Region\", \"Sub-Category\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.countplot(data=df, x=col, palette=\"viridis\")\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    \n",
    "    # Rotate x-axis labels for the 'Sub-Category' column\n",
    "    if col == \"Sub-Category\":\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=60)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f1fe9-13c9-4c72-bd9e-414ba737f5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab6fe4-e812-4c61-a27f-4fd5f4dc745b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19aca102-5608-45e7-a188-aec43a061c91",
   "metadata": {},
   "source": [
    "### Bivariate Analysis\n",
    "#### Conceptual Overview of Bivariate Analysis\n",
    "Bivariate analysis is a statistical technique used to analyze the relationship between two variables. It provides insights into how one variable changes in response to changes in another and is essential for identifying correlations, trends, and patterns. This type of analysis helps to understand the association, dependency, or interaction between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4b7c8-04ae-402f-837a-0e319820630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between Sales and Order Date\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df, x=\"Order Date\", y=\"Sales\", ci=None)\n",
    "plt.title(\"Sales Over Time\")\n",
    "plt.xlabel(\"Order Date\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.show()\n",
    "\n",
    "# Sales distribution across different categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x=\"Category\", y=\"Sales\", palette=\"viridis\")\n",
    "plt.title(\"Sales Distribution by Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.show()\n",
    "\n",
    "# Analyzing Sales by Segment and Region\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df, x=\"Segment\", y=\"Sales\", hue=\"Region\", palette=\"viridis\")\n",
    "plt.title(\"Sales by Segment and Region\")\n",
    "plt.xlabel(\"Segment\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523a7c1-2ba8-47b7-ac2c-cef73aed78d5",
   "metadata": {},
   "source": [
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498f6ae-68d8-47e6-8c90-aa028e48dc98",
   "metadata": {},
   "source": [
    "### Conceptual Overview of Time Series Analysis \n",
    "\n",
    "Time series analysis is a powerful statistical tool used to analyze and interpret data points collected or recorded at specific time intervals. Here's a brief overview tailored for our stakeholders:\n",
    "\n",
    "---\n",
    "\n",
    "#### What is Time Series Analysis?\n",
    "- **Definition**: Time series analysis involves examining data points gathered over time to identify patterns, trends, and seasonal variations.\n",
    "- **Objective**: The primary goal is to understand the underlying structure of the data, forecast future values, and make informed business decisions based on these insights.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key Components of Time Series Analysis\n",
    "1. **Trend**: Represents the long-term movement or direction in the data. It helps us understand if our sales are generally increasing, decreasing, or remaining stable over time.\n",
    "2. **Seasonality**: Captures recurring patterns or cycles in the data at regular intervals (e.g., monthly, quarterly). Identifying seasonality helps in planning for periods of high and low demand.\n",
    "3. **Residuals**: The irregular or random component that remains after removing the trend and seasonal effects. Analyzing residuals can highlight anomalies or events that affect sales unexpectedly.\n",
    "\n",
    "---\n",
    "\n",
    "#### Importance for Our Business\n",
    "- **Forecasting**: By understanding trends and seasonality, we can accurately forecast future sales, enabling better inventory management and financial planning.\n",
    "- **Strategic Planning**: Insights from time series analysis support strategic decision-making in marketing, resource allocation, and operational efficiency.\n",
    "- **Anomaly Detection**: Identifying residuals helps us detect unusual patterns that may indicate issues or opportunities, prompting timely corrective actions or leveraging favorable conditions.\n",
    "\n",
    "---\n",
    "\n",
    "Time series analysis is a vital tool in our analytics arsenal, providing us with actionable insights to drive our business forward with data-informed decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0e10a-9c0a-4e00-a304-28347842914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Order Date' to datetime format\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Create a deep copy of the DataFrame\n",
    "df_copy = df.copy(deep=True)\n",
    "\n",
    "# Set 'Order Date' as the index for the copied DataFrame\n",
    "df_copy.set_index('Order Date', inplace=True)\n",
    "\n",
    "#Resample by Month and Sum the Sales\n",
    "\n",
    "monthly_sales1 = df_copy.resample(\"M\")[\"Sales\"].sum()\n",
    "\n",
    "# Perform Seasonal Decomposition:\n",
    "result = seasonal_decompose(monthly_sales1, model = \"additive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13dad7-a682-4d32-9901-978aa0909a9c",
   "metadata": {},
   "source": [
    "## Decomposition Overview\n",
    "Time series decomposition is a technique used to break down a time series into its fundamental components: observed, trend, seasonal, and residuals. This helps in understanding the underlying patterns and structures in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2521d5-6422-4d09-b5ba-58980b74e425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5170e9-4e24-4d9b-868b-311eb64ec92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf6232-07be-4d5d-837b-de90067855b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6999b76-4e58-45ed-86f0-7e78e822c890",
   "metadata": {},
   "source": [
    "### Observed Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9279ca-037e-457b-88dd-336f2aafb8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed\n",
    "plt.figure(figsize = (16,6))\n",
    "plt.plot(result.observed, label='Observed', marker='o')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Observed')\n",
    "plt.xticks(result.observed.index, result.observed.index.strftime('%b'), rotation=45)\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb541505-a60c-42d0-9973-bd7f556b88c1",
   "metadata": {},
   "source": [
    "### Description:\n",
    "   The observed component represents the original time series data. It shows the actual sales values recorded over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617dce3-35d7-44c3-948c-eba65a04de0a",
   "metadata": {},
   "source": [
    "### Insight: \n",
    "This plot gives an overview of how sales have varied over the given period. It includes all components (trend, seasonal, and residual) combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d19b1-67bd-4db5-b556-823f9e09ba95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e6bdc-3a1a-432f-a186-311869008efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea357e9-4e81-4751-b445-41896ab53803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cf09e31-3ce8-47ed-b4f5-5806813a5b64",
   "metadata": {},
   "source": [
    "### Trend Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e1e6d-8389-415b-8b10-52593d57a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend\n",
    "plt.figure(figsize = (16,6))\n",
    "plt.plot(result.trend, label='Trend', color='orange', marker='o')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Trend')\n",
    "plt.xticks(result.trend.index, result.trend.index.strftime('%b'), rotation=45)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d2569-9e3a-4897-80b4-18a7cfbedb10",
   "metadata": {},
   "source": [
    "### Description:\n",
    "The trend component shows the long-term progression in the data. It highlights the general direction in which the time series is moving over a longer period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514d7aa-d6ce-4942-b543-9c57b03a78f5",
   "metadata": {},
   "source": [
    "### Insights \n",
    "\n",
    "The trend component of our time series analysis provides valuable insights into the long-term movement of our sales data. Here are the key takeaways for strategic decision-making:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Long-Term Growth\n",
    "- **Upward Trajectory**: The trend component reveals a smooth and consistent upward trajectory in our sales, confirming a positive growth trend over the analyzed period. This indicates sustained improvement in our business performance.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Strategic Implications\n",
    "- **Market Expansion**: The continuous growth in sales could be a result of successful market expansion efforts, suggesting that our strategies to enter new markets or expand our product lines are paying off.\n",
    "- **Increased Demand**: The upward trend might also reflect an overall increase in demand for our products or services. This insight can inform our production planning and supply chain management to meet the growing demand.\n",
    "- **Successful Marketing**: Effective marketing strategies and campaigns are likely contributing to this positive trend. Understanding this can help us allocate resources efficiently and continue investing in high-impact marketing initiatives.\n",
    "---\n",
    "By leveraging these insights, we can reinforce our growth strategies and ensure sustained success in the marketplace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb81f18-bec8-40f3-9a92-15ece2686f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e01df5-bb38-4738-9505-1d0bafe2ecab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b8e3c-fa43-4e59-a0f4-3ccbd293c220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e9da447-9799-42ee-997b-8349bd2431b7",
   "metadata": {},
   "source": [
    "### Seasonal Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4779b-c6fa-4165-842e-46dae7565ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal\n",
    "plt.figure(figsize = (16,6))\n",
    "plt.plot(result.seasonal, label='Seasonal', color='green', marker='o')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Seasonal')\n",
    "plt.xticks(result.seasonal.index, result.seasonal.index.strftime('%b'), rotation=45)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdab98-2f2b-45a5-b6cf-444cdbbb574d",
   "metadata": {},
   "source": [
    "### Description:\n",
    "The seasonal component of a decomposed time series captures the repeating patterns or cycles that occur at regular intervals within the data. These patterns are often influenced by seasonal factors such as weather, holidays, or business cycles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9640d15-9ffd-4b43-b3c4-54cd6af61b81",
   "metadata": {},
   "source": [
    "### **Insights**\n",
    "\n",
    "The seasonal component of our time series analysis reveals key patterns in our sales data that repeat annually. Here are some important insights to consider for strategic planning:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Repetitive Patterns\n",
    "- **Regular Yearly Cycles**: The seasonal component displays consistent patterns that recur every year, underscoring the strong seasonality in our sales data. Recognizing these cycles is crucial for anticipating sales trends.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Seasonal Peaks and Troughs\n",
    "- **High Sales Periods**: Certain times of the year, such as mid-year and the end of the year, exhibit noticeable peaks in sales. These peaks could be attributed to seasonal events or holidays, suggesting opportunities for targeted promotions and inventory boosts.\n",
    "- **Low Sales Periods**: Conversely, there are periods with consistently lower sales. Identifying these troughs helps in optimizing resources and managing costs effectively.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Strategic Implications\n",
    "- **Inventory Management**: By understanding these seasonal patterns, we can better plan our inventory to meet expected demand, reducing the risk of overstocking or stockouts.\n",
    "- **Marketing Campaigns**: Timing our marketing efforts to align with seasonal peaks can maximize their impact and drive higher sales.\n",
    "- **Staffing Levels**: Anticipating busy periods allows for better staffing management, ensuring we have adequate resources to handle increased customer activity.\n",
    "\n",
    "---\n",
    "\n",
    "Leveraging these insights will enhance our ability to make data-driven decisions and optimize our operational efficiency throughout the year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a56bed6-58cf-4464-b224-5071d6d48fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b8ae6-2cdf-4466-ac21-ac361cfffc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab726e4-562c-4ba8-97d8-74c1c97eb9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b859a21a-1d24-4a1b-a500-07c7e56ab75b",
   "metadata": {},
   "source": [
    "## **Residuals Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2592ab6-0c25-4344-a5e4-0b4a0ab305a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "plt.figure(figsize = (16,6))\n",
    "plt.plot(result.resid, label='Residuals', color='red', marker='o')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Residuals')\n",
    "plt.xticks(result.resid.index, result.resid.index.strftime('%b'), rotation=45)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5eb13-bda6-4f8a-aaa3-1d28d657a933",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###  Description:\n",
    "The residual component represents the remaining part of the time series after removing the trend and seasonal components. It    captures the irregular or random fluctuations in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf2ec6-ec6f-4661-97a1-0d5d15bd14e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "### **Insight**\n",
    "The residual plot from our time series analysis highlights the variability in our data that is not explained by the trend and seasonal components. Here are some key insights that are crucial for our business strategy:\n",
    "\n",
    "\n",
    "---\n",
    "#### Identification of Irregular Patterns and Anomalies\n",
    "\n",
    "- **Significant Positive Spikes**:\n",
    "  - **July and November (First Year)**: These months show notable positive spikes in the residuals, indicating that our actual sales were significantly higher than expected. This suggests that there may have been successful marketing campaigns, seasonal promotions, or other favorable events during these months that drove sales beyond our baseline trend and seasonal expectations.\n",
    "\n",
    "- **Significant Negative Spikes**:\n",
    "  - **August, December, and March**: These months display prominent negative spikes, suggesting that actual sales were considerably lower than expected. Potential reasons could include market downturns, stock shortages, increased competition, or ineffective promotional strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fd128-3c98-4234-a630-07d9a382ba1b",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "By decomposing the time series and visualizing each component separately, we gain a deeper understanding of the underlying patterns in the data. This allows for more informed decision-making and better forecasting. The enhanced visualizations, including monthly labels and markers, improve readability and provide clearer insights into the data trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6f6ab-b26b-40c5-8e89-e5a312961ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b537b78f-099b-47cf-a0e4-6d768cab43e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225df8cb-fb23-40dd-8a7e-1e127c5128ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f73f6e1-28f1-44d8-9050-960ce55ea975",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd9ff4-dbad-457e-bfcb-a2b451a5269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features like 'Order Month', 'Order Year', 'Delivery Time'\n",
    "df[\"Order Month\"] = df[\"Order Date\"].dt.month\n",
    "df[\"Order Year\"] = df[\"Order Date\"].dt.year\n",
    "df[\"Delivery Time\"] = (df[\"Ship Date\"] - df[\"Order Date\"]).dt.days\n",
    "\n",
    "# Explore the new features\n",
    "print(df[[\"Order Month\", \"Order Year\", \"Delivery Time\"]].head())\n",
    "\n",
    "# Plotting Delivery Time distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"Delivery Time\"], kde=True, color=\"purple\")\n",
    "plt.title(\"Delivery Time Distribution\")\n",
    "plt.xlabel(\"Delivery Time (days)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdf2fb-e6f3-4df8-8eb3-cbb5528788ad",
   "metadata": {},
   "source": [
    "### **Outlier Detection and Treatment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56154fad-4200-4ced-8fe8-48fcc57f1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for detecting outliers in Sales\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(df[\"Sales\"])\n",
    "plt.title(\"Boxplot of Sales\")\n",
    "plt.xlabel(\"Sales Value\")\n",
    "plt.ylabel(\"Count of Sales Observations\")\n",
    "plt.show()\n",
    "\n",
    "# Treat outliers if necessary (e.g., capping, transformation)\n",
    "# Example: Capping Sales at the 99th percentile\n",
    "upper_limit = df[\"Sales\"].quantile(0.99)\n",
    "df[\"Sales\"] = np.where(df[\"Sales\"] > upper_limit, upper_limit, df[\"Sales\"])\n",
    "\n",
    "# Verify the capping effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(df[\"Sales\"])\n",
    "plt.title(\"Boxplot of Sales After Capping\")\n",
    "plt.xlabel(\"Sales Value\")\n",
    "plt.ylabel(\"Count of Sales Observations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9d7f7-2a53-440e-aade-292d3ff80dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993acf24-fc82-46cf-9980-3ce3021fe0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43f5a74b-1eac-477c-a9f4-7886896882fc",
   "metadata": {},
   "source": [
    "## **Statistics Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa489e4-dc16-4d1b-8396-3a94745e644d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summarize key insights from the EDA\n",
    "summary = df.describe(include=\"all\")\n",
    "print(summary)\n",
    "\n",
    "# For example, sales trends, top-performing segments, impact of delivery time, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46a848-2ff1-474c-94ba-9ed11d50e672",
   "metadata": {},
   "source": [
    "## **Insights from the statistical summary** ##\n",
    "#### 1. **Order and Shipping Details**\n",
    "- **Order Date**: Sales records are evenly distributed over the period, with a median order date of **June 26, 2017**.\n",
    "- **Shipping**:\n",
    "- Shipping times range from **0 to 7 days**, with an average delivery time of **4 days**.\n",
    "- The most frequently used shipping method is **Standard Class** (chosen for 59.7% 0f orders).\n",
    "---\n",
    "\n",
    "\n",
    "#### 2. **Customer Insights**\n",
    "- **Customers**: \n",
    "  - The dataset includes **793 unique customers**. \n",
    "  - One customer, identified as `WB-21850`, placed the most orders (**35 orders**), indicating potential loyalty.\n",
    "- **Regions and Cities**:\n",
    "  - The sales span **4 regions**, with the **West region** accounting for **32.7%** of the orders.\n",
    "  - Sales are concentrated in **New York City**, which appears **891 times** in the dataset.\n",
    "---\n",
    "\n",
    "\n",
    "#### 3. **Product Analysis**\n",
    "- **Categories and Sub-categories**:\n",
    "  - Most sales fall under the category **Office Supplies** (**60.3% of transactions**), with **Binders** being the top-selling sub-category.\n",
    "  - **Top Product** The most frequently sold product is **Staple envelope** (sold **47 times**).\n",
    "  - **Product Diversity**: The dataset contains **1,848 unique products**, showcasing a wide variety of offerings.\n",
    "---\n",
    "\n",
    "#### 4. **Sales Performance**\n",
    "- **Sales Value**:\n",
    "   - The average sales value is **209.26**, but the median is significantly lower at **$54.38**, indicating the presence of a few high-value transactions.\n",
    "   - Sales range from**$0.44 to $2,456.61**, with high variability as shown by a standard deviation of **$392.90**.\n",
    "- **High Value Transactions**:\n",
    " - A small percentage of sales contribute significantly to total revenue, which may warrant target analysis to indetify opportunities for growth.\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1256e32-c79d-4fb3-9aca-89fa993b978c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae52039-c75b-4a6e-a834-39b33ee2d7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a377f7-6cb6-4116-8c41-537bafe62d1a",
   "metadata": {},
   "source": [
    "### **Visualization of Key Insights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830aa19-8025-4354-840c-ef9c2bd2f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Top 10 Products by Sales\n",
    "top_products = df.groupby(\"Product Name\")[\"Sales\"].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_products.plot(kind=\"bar\", color=\"green\")\n",
    "plt.title(\"Top 10 Products by Sales\")\n",
    "plt.xlabel(\"Product Name\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.show()\n",
    "\n",
    "# Example: Sales by Region\n",
    "sales_by_region = df.groupby(\"Region\")[\"Sales\"].sum()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sales_by_region.plot(kind=\"pie\", autopct=\"%1.1f%%\", startangle=140)\n",
    "plt.title(\"Sales Distribution by Region\")\n",
    "plt.ylabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c8676-a360-4377-9d3d-51ccde302b88",
   "metadata": {},
   "source": [
    "## **Cohort Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119dd4a9-cbee-44d5-910f-3f9826184843",
   "metadata": {},
   "source": [
    "### Conceptual Overview of Cohort Analysis\n",
    "\n",
    "Cohort Analysis is a powerful analytical technique used to understand the behavior of groups of customers who share a common characteristic or experience within a specific time period. This method helps us to identify patterns and trends over time, enabling more informed decision-making. \n",
    "\n",
    "#### Key Points:\n",
    "1. **Grouping by Common Characteristics**:\n",
    "   - Customers are grouped into cohorts based on shared attributes, such as the month of their first purchase or the acquisition channel.\n",
    "\n",
    "2. **Tracking Over Time**:\n",
    "   - By analyzing the behavior of these cohorts over subsequent periods, we can observe how their engagement, retention, and purchasing patterns evolve.\n",
    "\n",
    "3. **Insightful Metrics**:\n",
    "   - Cohort analysis allows us to measure critical metrics such as retention rates, average order value, and customer lifetime value for different cohorts.\n",
    "\n",
    "4. **Strategic Benefits**:\n",
    "   - This analysis helps in identifying successful strategies for customer acquisition and retention, understanding the impact of marketing campaigns, and making data-driven decisions to enhance customer experience and business growth.\n",
    "\n",
    "By leveraging cohort analysis, we gain a deeper understanding of our customer base, enabling us to tailor our strategies to better meet their needs and drive sustainable growth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f21d5c-23a5-46d3-a2ed-eaf6b432b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure order date is in datetime format\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n",
    "\n",
    "#Create a new column for the cohort month based on the first purchase of each customer\n",
    "df[\"CohortMonth\"] = df.groupby(\"Customer ID\")[\"Order Date\"].transform(\"min\").dt.to_period(\"M\")\n",
    "\n",
    "# Create a new column for the order month \n",
    "df[\"OrderMonth\"] = df[\"Order Date\"].dt.to_period(\"M\")\n",
    "\n",
    "# Group by cohort month and order month to calculate thr number of unique customers in each cohort over time\n",
    "cohort_data = df.groupby([\"CohortMonth\",\"OrderMonth\"]).agg({\"Customer ID\": \"nunique\"}).reset_index()\n",
    "\n",
    "# Create a column for the cohort index\n",
    "cohort_data[\"CohortIndex\"] = (cohort_data[\"OrderMonth\"] - cohort_data[\"CohortMonth\"]).apply(lambda x: x.n)\n",
    "\n",
    "# Pivot the data for the retention matrix \n",
    "cohort_pivot = cohort_data.pivot_table(index = \"CohortMonth\", values = \"Customer ID\")\n",
    "\n",
    "# Calculate the retention rate \n",
    "cohort_size = cohort_pivot.iloc[:, 0]\n",
    "retention_matrix = cohort_pivot.divide(cohort_size, axis = 0)\n",
    "\n",
    "# Plot the retention matrix \n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.heatmap(retention_matrix, annot = True, fmt = \".0%\", cmap = \"YlGnBu\")\n",
    "plt.title(\"Cohort Analysis - Retention Rate\")\n",
    "plt.ylabel(\"Cohort Month\")\n",
    "plt.xlabel(\"Cohort Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16ab947-9f85-4881-9c49-87852c064220",
   "metadata": {},
   "source": [
    "\n",
    "## **Insights from the Cohort Analysis Retention Heatmap**\n",
    "The retention heatmap provides a visual representation of customer retention over different cohorts and time periods. Here are the key insights derived from the heatmap:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd8b631-8182-4efb-8ea0-194627acf015",
   "metadata": {},
   "source": [
    "#### 1. Consistent Retention Rate:\n",
    "\n",
    "The heatmap indicates a consistent 100% retention rate across all cohorts and time periods.\n",
    "This suggests that once customers make their first purchase, they continue to engage with our products or services consistently over time.\n",
    "#### 2. Strong Customer Loyalty:\n",
    "\n",
    "The uniform retention rate implies strong customer loyalty and satisfaction.\n",
    "Customers are likely finding significant value in our offerings, leading to their continued patronage.\n",
    "#### 3. Stable Customer Base:\n",
    "\n",
    "The stability in retention rates across different cohorts suggests a stable customer base.\n",
    "This stability is crucial for forecasting future revenue and planning business strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf65eb82-9ae2-492d-a5c8-aaa22414d292",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "By leveraging these insights, we can continue to build a loyal customer base, enhance customer satisfaction, and drive sustainable business growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9971d-1990-40ac-817e-6212697c5067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165f00c-a087-4e2e-8363-87fdc2c10011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b2ac0c-8b84-4bd6-8917-aea49d97cae9",
   "metadata": {},
   "source": [
    " ## **RFM Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cf834-c4e9-4488-b41b-95387e2be21d",
   "metadata": {},
   "source": [
    "### Conceptual Overview of RFM Analysis\n",
    "RFM Analysis is a marketing technique used to quantitatively rank and segment customers based on their purchasing behavior. RFM stands for Recency, Frequency, and Monetary value:\n",
    "\n",
    "#### 1. Recency (R): \n",
    "How recently a customer made a purchase.\n",
    "#### 2. Frequency (F):\n",
    "How often a customer makes a purchase.\n",
    "#### 3. Monetary Value (M): \n",
    "How much money a customer spends on purchases.\n",
    "This analysis helps businesses identify their best customers, understand customer behavior, and tailor marketing strategies accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11531a7-030c-43e5-aaa0-22ab643c534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eeda8d-2b83-4c23-81a7-955756a4be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure order date is in datetime\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"])\n",
    "\n",
    "# Define the current date \n",
    "NOW = dt.datetime.now()\n",
    "\n",
    "# Aggregate data by customer_id\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"Order Date\": lambda x: (NOW - x.max()).days,\n",
    "    \"Order ID\": \"count\",\n",
    "    \"Sales\": \"sum\"\n",
    "})\n",
    "\n",
    "# Rename columns\n",
    "rfm.columns = [\"Recency\", \"Frequency\", \"Monetary\"]\n",
    "\n",
    "# Assign RFM scores \n",
    "rfm[\"R\"] = pd.qcut(rfm[\"Recency\"], 4, labels = [\"1\", \"2\", \"3\", \"4\"])\n",
    "rfm[\"F\"] = pd.qcut(rfm[\"Frequency\"], 4, labels = [\"4\", \"3\", \"2\", \"1\"])\n",
    "rfm[\"M\"] = pd.qcut(rfm[\"Monetary\"], 4, labels = [\"4\", \"3\", \"2\", \"1\"])\n",
    "\n",
    "# Create RFM_score column\n",
    "rfm[\"RFM_Score\"] = rfm[\"R\"].astype(str) + rfm[\"F\"].astype(str) +  rfm[\"M\"].astype(str)\n",
    "\n",
    "# Define a function to segment customers\n",
    "def rfm_segment(df):\n",
    "    if df[\"RFM_Score\"] == \"111\":\n",
    "        return \"Champions\"\n",
    "    elif df[\"RFM_Score\"] in [\"112\", \"121\", \"122\", \"211\", \"212\", \"221\"]:\n",
    "        return \"Loyal Customers\"\n",
    "    elif df[\"RFM_Score\"] in [\"113\", \"131\", \"132\", \"213\", \"231\"]:\n",
    "        return \"Potential Loyalists\"\n",
    "    elif df[\"RFM_Score\"] in [\"114\", \"141\", \"142\", \"214\", \"241\"]:\n",
    "        return \"New Customers\"\n",
    "    elif df[\"RFM_Score\"] in [\"123\", \"132\", \"223\", \"232\", \"233\"]:\n",
    "        return \"Promising Customers\"\n",
    "    elif df[\"RFM_Score\"] in [\"213\", \"231\", \"312\", \"313\"]:\n",
    "        return \"Need Attention\"\n",
    "    elif df[\"RFM_Score\"] in [\"313\", \"323\", \"331\", \"332\"]:\n",
    "        return \"At Risk\"\n",
    "    elif df[\"RFM_Score\"] in [\"411\", \"421\", \"431\"]:\n",
    "        return \"Can’t Lose Them\"\n",
    "    elif df[\"RFM_Score\"] in [\"414\", \"424\", \"434\"] :\n",
    "        return \"Hibernating\"\n",
    "    elif df[\"RFM_Score\"] in [\"443\", \"444\"]:\n",
    "        return \"Lost Customers\"\n",
    "    else:\n",
    "        return \"Low-Value Customers\"\n",
    "    \n",
    "\n",
    "\n",
    "# Apply the function\n",
    "rfm[\"Segment\"] = rfm.apply(rfm_segment, axis = 1)\n",
    "\n",
    "# Print RFM table \n",
    "print(rfm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d0d3a-8369-4bb7-9d51-a0b329b83281",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot histograms for R, F, and M with segment names\n",
    "plt.figure(figsize = (18, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(rfm[\"Recency\"], bins = 20, kde = True)\n",
    "plt.title(\"Recency Distribution\")\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(rfm[\"Frequency\"], bins = 20, kde = True)\n",
    "plt.title(\"Frequency Distribution\")\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(rfm[\"Monetary\"], bins = 20, kde = True)\n",
    "plt.title(\"Monetary Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d33f4-0caa-45ff-a56c-23bf91877524",
   "metadata": {},
   "source": [
    "## **Insights from RFM Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1987143-1971-442a-81c1-9cbfe1599d96",
   "metadata": {},
   "source": [
    "###  Recency, Frequency, and Monetary Distributions\n",
    "#### 1. Recency Distribution:\n",
    "- Most customers have made recent purchases, indicating a high level of engagement.\n",
    "- The distribution is skewed to the right, suggesting that while many customers are recent, there are some who haven't engaged in a while.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 2. Frequency Distribution:\n",
    "- The majority of customers have a lower purchase frequency, with a significant drop-off after a few purchases. This indicates a need for strategies to increase repeat purchases.\n",
    "- The right skewness suggests that only a small segment of customers makes frequent purchases.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Monetary Distribution:\n",
    "- Most customers have relatively low total spend, with a few high-value customers driving a significant portion of sales.\n",
    "- The distribution highlights the presence of top-tier customers who contribute a disproportionately large amount to revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac86ba-f78c-4839-af95-44950aa4d233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e14b40-8554-4152-a79e-54c1f27e05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_counts = rfm[\"Segment\"].value_counts().reset_index()\n",
    "rfm_counts.columns = [\"Segment\", \"Count\"]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# No need for 'hue' in sns.barplot\n",
    "sns.barplot(data=rfm_counts, x=\"Segment\", y=\"Count\", hue = \"Segment\", palette=\"viridis\")\n",
    "\n",
    "plt.title(\"RFM Segments\")\n",
    "plt.xlabel(\"Segment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add legend manually\n",
    "plt.legend(title=\"Segment\", loc=\"upper right\", labels=rfm_counts[\"Segment\"])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f139968-ab8a-4c34-b4c5-15a7d85090ec",
   "metadata": {},
   "source": [
    "\n",
    "## Insights from RFM Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ee7a8-0cc2-426b-9698-8ee6abbbb6d3",
   "metadata": {},
   "source": [
    "### RFM Segments Bar Plot \n",
    "####  Segment Distribution:\n",
    "- The plot shows a clear dominance of \"Low-value customers,\" who consistently engage with the brand and contribute significantly to revenue.\n",
    "- \"low-value customers\" and \"Loyal Customers\" make up the largest segments, suggesting that a core group of customers drives a majority of sales.\n",
    "-  Smaller segments like \"At Risk Customers\" and \"Need Attention\" highlight areas where the business might need to re-engage customers who are drifting away.\n",
    "- The distribution also suggests potential opportunities in converting customers from smaller, less engaged segments into more loyal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1e605-85df-4483-9eea-87823c9102b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm1_heatmap = rfm.pivot_table(index = \"R\", columns= \"F\", values = \"Monetary\", aggfunc =  \"mean\").astype(int)\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    rfm1_heatmap, annot = True, fmt = \"d\", cmap = \"coolwarm\", linewidths = 0.5, cbar_kws = {\"label\": \"Mean Monetary Value ($)\"}\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\"RFM Heatmap: Average Monetary Value by Recency and Frequency\", fontsize = 16)\n",
    "plt.xlabel(\"Frequency (1= High Frequency, 4 = Low Frequency)\", fontsize = 12)\n",
    "plt.ylabel(\"Recency (1= Recent purchase, 4 = Long Time ago)\", fontsize = 12)\n",
    "\n",
    "ax.set_xticklabels([\"4 (Low)\", \"3\", \"2\", \"1 (High)\"], fontsize=10)\n",
    "ax.set_yticklabels([\"4 (Old)\", \"3\", \"2\", \"1 (Recent)\"], fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e7b478-4bee-4e87-abe5-405401ad2f17",
   "metadata": {},
   "source": [
    "## **Insights from RFM Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cd183-3d3e-483e-b25f-a0ffaf8ecf93",
   "metadata": {},
   "source": [
    "### RFM Heatmap Insights:\n",
    "- The heatmap visually represents the intersection of Recency, Frequency, and Monetary scores.\n",
    "- Customers with high Recency and Frequency scores, who are also big spenders, form the most valuable segment.\n",
    "- There is a clear concentration of high monetary values in segments with high Frequency and Recency scores, emphasizing the importance of frequent and recent engagement.\n",
    "- The segmentation further identifies potential areas of focus, such as increasing the monetary value of customers with high Recency but low Frequency scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ea158-7bea-4b46-95aa-848c3757ace0",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "These insights collectively guide the business in understanding customer behavior, identifying key segments, and tailoring strategies to enhance customer retention, loyalty, and overall profitability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5babcb-173f-43a2-8f18-3211064812a6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3c18d-187e-46ce-a73e-847813d51d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc17a2b-fdbf-41d8-a2b5-3c2bb08fe010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9851c65-9736-48ac-8171-efd52ba8cdbc",
   "metadata": {},
   "source": [
    "## **Conceptual Overview of Customer Lifetime Value (CLV) Prediction**\n",
    "Customer Lifetime Value (CLV) is a crucial metric that estimates the total revenue a business can expect from a customer over the entire duration of their relationship. Predicting CLV helps businesses allocate resources efficiently, optimize marketing strategies, and focus on high-value customer segments.\n",
    "\n",
    "### Key Concepts:\n",
    "#### 1. Historical Data Analysis:\n",
    "\n",
    "- CLV prediction begins with analyzing past customer behavior, including purchase frequency, average order value, and customer retention rates.\n",
    "- Historical data provides a foundation for understanding trends and patterns in customer behavior.\n",
    "\n",
    "  ---\n",
    "#### 2. Segmentation:\n",
    "\n",
    "- Customers are often segmented based on Recency, Frequency, and Monetary (RFM) metrics. Each segment may exhibit different spending patterns and lifetimes.\n",
    "- Segmentation helps in creating more accurate and tailored CLV predictions for different customer groups.\n",
    "Predictive Modeling:\n",
    "\n",
    "- Advanced statistical methods or machine learning models are used to predict future behavior based on historical data.\n",
    "- Models like Cohort Analysis, Logistic Regression, and Survival Analysis are commonly used in CLV prediction.\n",
    "\n",
    "  ---\n",
    "#### 3. Discount Rate:\n",
    "\n",
    "- The future value of money is discounted to present value terms using a discount rate. This accounts for the time value of money in CLV predictions.\n",
    "\n",
    "  ---\n",
    "#### 4. Retention Rate:\n",
    "\n",
    "- A critical factor in CLV prediction, as higher retention rates typically lead to higher CLV.\n",
    "- Businesses can improve retention by understanding what drives customer loyalty and addressing pain points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afa66d-a3b7-44b4-b1db-1e7b5ee2ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b864e11a-9c8b-4a0d-bf7b-777166cf306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a  reference date for the Recency calculation \n",
    "NOW = df[\"Order Date\"].max() + pd.Timedelta(days = 1)\n",
    "\n",
    "# Aggregate data by the customer ID to calculate Recency , Frequency, and Monetary\n",
    "rfm = df.groupby(\"Customer ID\").agg({\n",
    "    \"Order Date\": lambda x: (NOW - x.max()).days, # Recency\n",
    "    \"Order ID\": \"nunique\", # Frequency\n",
    "    \"Sales\": \"sum\" # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "rfm.columns = [\"Customer ID\", \"Recency\", \"Frequency\", \"Monetary\"]\n",
    "\n",
    "# Add Average Order Value and Customer LIfetime Duration\n",
    "rfm[\"Avg_Order_Value\"] = rfm[\"Monetary\"]/ rfm[\"Frequency\"]\n",
    "rfm[\"Customer_Lifetime_Duration\"] = df.groupby(\"Customer ID\")[\"Order Date\"].apply(lambda x: (x.max() - x.min()).days).reset_index(drop = True)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "rfm[[\"Recency\", \"Frequency\", \"Monetary\", \"Avg_Order_Value\", \"Customer_Lifetime_Duration\"]] = scaler.fit_transform(\n",
    "    rfm[[\"Recency\", \"Frequency\", \"Monetary\", \"Avg_Order_Value\", \"Customer_Lifetime_Duration\"]]\n",
    "    \n",
    ")\n",
    "\n",
    "x = rfm[[\"Recency\", \"Frequency\", \"Monetary\", \"Avg_Order_Value\", \"Customer_Lifetime_Duration\"]]\n",
    "y = rfm[\"Monetary\"]  # Using Monetary as a proxy for CLV\n",
    "\n",
    "# split the data into training testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Train a Gradient Boosting Regressor \n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc8256-8849-4ada-801b-efb0298bfdde",
   "metadata": {},
   "source": [
    "### **Conceptual Overview of Model Evaluation**\n",
    "Model evaluation is a critical step in the machine learning process, used to assess how well a model performs and how effectively it generalizes to new, unseen data. It involves applying various metrics and techniques to understand the accuracy, robustness, and reliability of the model.\n",
    "\n",
    "#### Key Concepts:\n",
    "#### 1. Training vs. Testing Data:\n",
    "\n",
    "- Training Data: The data used to train the model.\n",
    "- Testing Data: The data used to evaluate the model's performance on unseen examples, providing an estimate of how well the model will perform in the real world.\n",
    "\n",
    "---\n",
    "#### 2. Evaluation Metrics:\n",
    "\n",
    "- Accuracy: Measures the proportion of correct predictions out of all predictions. It's commonly used in classification tasks.\n",
    "- Precision, Recall, F1-Score: Metrics used in classification, particularly in cases of imbalanced data. Precision measures the accuracy of positive predictions, recall measures the ability to find all positive instances, and F1-Score balances precision and recall.\n",
    "- Mean Absolute Error (MAE): The average of absolute differences between predicted and actual values, commonly used in regression tasks.\n",
    "- Root Mean Squared Error (RMSE): The square root of the average of squared differences between predicted and actual values, providing a measure of prediction error.\n",
    "- R-squared (R²): Indicates how well the model explains the variance in the target variable, commonly used in regression analysis.\n",
    "\n",
    "---\n",
    "#### Cross-Validation:\n",
    "\n",
    "A technique where the dataset is split into multiple subsets, and the model is trained and tested on different combinations of these subsets. This helps in ensuring that the model's evaluation is robust and not dependent on a single train-test split.\n",
    "\n",
    "---\n",
    "#### Overfitting and Underfitting:\n",
    "\n",
    "- Overfitting: When the model performs well on the training data but poorly on new data because it has learned the noise or random fluctuations in the training data.\n",
    "- Underfitting: When the model is too simple and fails to capture the underlying patterns in the data, leading to poor performance on both training and testing data.\n",
    "\n",
    "---\n",
    "### Conclusion:\n",
    "Model evaluation is essential for understanding the effectiveness of a machine learning model. By using appropriate metrics and techniques, you can ensure that the model not only performs well on the training data but also generalizes effectively to new, unseen data, leading to reliable and accurate predictions in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586b756f-8077-47d2-a2d8-40345198ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate MAE, RMSE, and R²\n",
    "mae = round(mean_absolute_error(y_test, y_pred))\n",
    "rmse = round(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "r2 = round(r2_score(y_test, y_pred))\n",
    "\n",
    "# Print the results \n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'R² (R-squared): {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bec76b-45bb-470c-8f5a-969b94e375d6",
   "metadata": {},
   "source": [
    "### Model Evaluation Summary\n",
    "The evaluation metrics of our predictive model indicate an exceptionally high level of accuracy:\n",
    "\n",
    "#### Mean Absolute Error (MAE): 0\n",
    "The MAE of 0 suggests that the average difference between the predicted and actual values is zero, meaning our model's predictions are perfectly aligned with the actual outcomes.\n",
    "\n",
    "---\n",
    "#### Root Mean Squared Error (RMSE): 0\n",
    "Similarly, the RMSE of 0 reinforces that there is no deviation between the predicted and actual values, highlighting the model's precision.\n",
    "\n",
    "---\n",
    "#### R² (R-squared): 1\n",
    "An R² value of 1 indicates that the model explains 100% of the variance in the target variable. This is a perfect score, demonstrating that the model is fully capable of predicting the target variable based on the features used.\n",
    "\n",
    "---\n",
    "### Conclusion:\n",
    "The model's evaluation results suggest that it is perfectly accurate for the given dataset. However, it's important to consider whether this is realistic or if there might be issues such as overfitting, where the model is too closely tailored to the training data and may not perform as well on unseen data. Further validation or a review of the data might be needed to ensure that these results are generalizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d763cd-e982-4a4a-a176-3a1181ff5dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae6e57-3ec2-41ac-b23d-ee5ba32a9d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd19a6b-2b0b-418a-a9c1-09b0fd8d02cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce4f54d-d64b-4644-8691-c5c02aeebf76",
   "metadata": {},
   "source": [
    "## **Overview of Actual vs. Predicted Values**\n",
    "\n",
    "In predictive modeling, the comparison between actual values (the true values from your data) and predicted values (the values estimated by your model) is crucial for evaluating the model's performance.\n",
    "\n",
    "---\n",
    "### Actual Values\n",
    "These are the ground truth values from the dataset. For example, in a sales prediction model, the actual values would be the real sales figures recorded.-----\n",
    "### Predicted Values\n",
    "These are the values that your model estimates based on the input features. In the sales prediction example, these would be the sales figures predicted by the model for the same data points.\n",
    "\n",
    "---\n",
    "### Importance of Comparing Actual vs. Predicted Values\n",
    "\n",
    "- **Model Accuracy:** The closer the predicted values are to the actual values, the more accurate the model is. Various metrics, such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R² (R-squared), are used to quantify this closeness.\n",
    "\n",
    "- **Error Analysis:** By comparing actual and predicted values, you can identify where the model is making large errors and investigate possible reasons. This can help in refining the model.\n",
    "\n",
    "- **Model Validation:** A consistent pattern of close actual vs. predicted values across a validation or test dataset indicates that the model generalizes well and is likely not overfitting.\n",
    "\n",
    "- **Business Implications:** For stakeholders, understanding the difference between actual and predicted values helps in assessing the reliability of the model’s forecasts, which is critical for decision-making.\n",
    "\n",
    "---\n",
    "### Visualizing Actual vs. Predicted Values\n",
    "\n",
    "- **Scatter Plots:** A scatter plot of actual vs. predicted values with a line representing perfect prediction (y = x) can show how closely the predictions align with reality.\n",
    "  \n",
    "- **Residual Plots:** Residuals (the differences between actual and predicted values) can be plotted to check for patterns, helping to detect issues like heteroscedasticity or non-linearity in the model.\n",
    "---\n",
    "### Summary\n",
    "\n",
    "Comparing actual vs. predicted values is a key step in validating a model’s performance, ensuring that it meets the desired accuracy and reliability for practical use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1521ec6-a646-4e96-b3ef-c5d5c0b8c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Actual vs Predicted CLV\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\", lw = 2)\n",
    "plt.ylabel(\"Actual CLV\")\n",
    "plt.xlabel(\"Predicted CLV\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701f5f7-55a4-4d9b-8961-1f8b567b1e47",
   "metadata": {},
   "source": [
    "### **Insights from the Actual vs. Predicted CLV Plot**\n",
    "\n",
    "#### High Accuracy:\n",
    "- The plot shows that the predicted Customer Lifetime Value (CLV) closely aligns with the actual CLV values. Most of the data points lie directly on or very near the diagonal line, indicating that the model's predictions are highly accurate.\n",
    "\n",
    "#### Model Performance:\n",
    "- The close alignment between the actual and predicted values suggests that the model is performing exceptionally well. This level of accuracy is confirmed by the nearly perfect fit of the data points to the diagonal, where each predicted value matches its corresponding actual value.\n",
    "\n",
    "#### Low Error Rates:\n",
    "- The minimal deviations of the points from the diagonal line indicate low error rates, further validating the model’s reliability in predicting CLV.\n",
    "\n",
    "#### Outliers:\n",
    "- There are a few data points that are slightly off the diagonal line, indicating small discrepancies between the predicted and actual CLV for these cases. These outliers could be analyzed further to understand any specific factors contributing to these prediction differences.\n",
    "\n",
    "#### Business Implications:\n",
    "- For stakeholders, this plot provides strong evidence that the model can be trusted for making business decisions based on predicted CLV. The model's ability to predict CLV with high accuracy can assist in more precise customer segmentation, targeted marketing, and revenue forecasting.\n",
    "\n",
    "Overall, this plot demonstrates that the model is robust, reliable, and ready for deployment in real-world scenarios where accurate CLV prediction is essential.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5883ee-a3d6-4cc7-860e-507341fc6997",
   "metadata": {},
   "source": [
    "## **Learning Curve** \n",
    "### Conceptual Overview of Learning Curves\n",
    "Learning curves are graphical representations used in machine learning to understand the model's performance as it learns from more data over time. They plot the training and validation (or testing) error rates against the number of training samples or iterations.\n",
    "\n",
    "#### Key Concepts:\n",
    "#### 1. Training Error:\n",
    "\n",
    "- The error (or loss) that the model makes on the training data.\n",
    "- As the model is exposed to more data, the training error typically decreases, showing that the model is learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Validation (Testing) Error:\n",
    "\n",
    "- The error that the model makes on a separate validation or testing dataset.\n",
    "- This helps in understanding how well the model generalizes to new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Typical Patterns:\n",
    "\n",
    "- Underfitting: Both training and validation errors are high and close to each other, indicating that the model is too simple to capture the underlying patterns in the data.\n",
    "- Overfitting: The training error is low, but the validation error remains high or increases, indicating that the model is too complex and is capturing noise in the training data rather than the true underlying pattern.\n",
    "- Good Fit: The training error decreases and stabilizes, while the validation error also decreases and stabilizes close to the training error, indicating a well-fitted model.\n",
    "Insights from Learning Curves:\n",
    "\n",
    "- Data Sufficiency: If both training and validation errors are high, increasing the model's complexity or collecting more data might be necessary.\n",
    "- Model Complexity: If the training error is much lower than the validation error, the model might be overfitting, and reducing its complexity (e.g., by pruning a decision tree or reducing the number of features) could help.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "Learning curves are a valuable tool for diagnosing issues like underfitting or overfitting in a model. By analyzing the curves, you can determine whether the model needs more data, a different level of complexity, or further tuning to achieve better generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c83f3-9bbb-4eb4-87a9-f2af5e80f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curve to check for overfitting\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, x, y, cv = 5)\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.plot(train_sizes, train_scores.mean(axis = 1), label = \"Training score\")\n",
    "plt.plot(train_sizes, test_scores.mean(axis = 1), label = \"Validation score\")\n",
    "plt.xlabel(\"Training Size\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02db6b9-b6b2-4dd2-89c7-2dd2ef00549e",
   "metadata": {},
   "source": [
    "## Insights from the Learning Curve for CLV Prediction\n",
    "- The learning curve provided offers valuable insights into the performance of our model for predicting Customer Lifetime Value (CLV). Below are the key takeaways that can be presented to the stakeholders:\n",
    "\n",
    "#### 1. High Training Score:\n",
    "\n",
    "- The training score remains consistently high across all training sizes, indicating that the model is very accurate on the training data. This suggests that the model has effectively learned the patterns within the training dataset.\n",
    "#### 2. Improvement in Validation Score:\n",
    "\n",
    "- The validation score starts lower but increases sharply as the training size grows, eventually stabilizing close to the training score. This trend indicates that the model improves its generalization ability as it is exposed to more data, suggesting that the model benefits from additional training samples.\n",
    "#### Potential Overfitting Concerns:\n",
    "\n",
    "- The fact that the training score is consistently at or near 1.0, combined with the validation score stabilizing slightly lower, could indicate a slight overfitting issue. While the model performs well on unseen data, the gap between training and validation scores implies that the model may be slightly too complex, capturing noise in the training data.\n",
    "#### Sufficiency of Training Data:\n",
    "\n",
    "- The learning curve suggests that the model has reached a point of stability, where additional training data yields diminishing returns in improving validation performance. This indicates that the current dataset is likely sufficient for this model, and significant gains in validation accuracy may not be achieved by merely increasing the training size.\n",
    "#### Model Reliability:\n",
    "\n",
    "- Overall, the model demonstrates strong performance, with the validation score approaching 1.0 as well. This indicates that the model can be reliably used for predicting CLV, as it generalizes well to new data.\n",
    "### Conclusion:\n",
    "The learning curve analysis shows that our CLV prediction model is performing well, but with some signs of potential overfitting. This insight suggests that while the model is effective, further refinement or regularization techniques might improve its ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca4d51-f503-48dd-b08d-34a30a689a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43abb0c8-b15e-4831-9cfe-806383c051ab",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "**Feature importance** is a key concept in machine learning that helps identify which input features (variables) have the most influence on the output of a predictive model. It is crucial for understanding how a model makes decisions, enabling better interpretability and insights into the data.\n",
    "\n",
    "### Definition:\n",
    "Feature importance quantifies the contribution of each feature to the model’s predictions. Higher importance indicates a stronger relationship between the feature and the target variable.\n",
    "\n",
    "### Calculation Methods:\n",
    "- **Model-Based:** Many algorithms, like decision trees, random forests, and gradient boosting, inherently provide feature importance scores based on how often and effectively a feature is used to split data.\n",
    "- **Permutation Importance:** Measures the change in model performance when the values of a feature are randomly shuffled, highlighting its impact on the predictions.\n",
    "- **SHAP Values:** Provides a consistent and interpretable measure of feature importance by considering the contribution of each feature across all possible feature combinations.\n",
    "\n",
    "### Applications:\n",
    "- **Model Interpretation:** Helps data scientists and stakeholders understand which features are driving predictions, supporting transparency and trust in the model.\n",
    "- **Feature Selection:** Enables the reduction of dimensionality by focusing on the most important features, leading to simpler, faster, and potentially more accurate models.\n",
    "\n",
    "Overall, **feature importance** is a critical tool in both developing effective models and ensuring their interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1809d-6d6f-44f8-8845-9fc077ed87ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature importance\n",
    "importances = model.feature_importances_\n",
    "features = x.columns\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.barplot(x = importances, y = features)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c20e01-ee42-4521-acda-cf38192ee123",
   "metadata": {},
   "source": [
    "### **Feature Importance Insights**\n",
    "\n",
    "The feature importance plot highlights which variables most significantly impact our target prediction. The key insights are as follows:\n",
    "\n",
    "1. **Monetary**: This feature is by far the most important predictor in our model. It indicates that the total amount spent by a customer is the primary driver of the predicted outcome. This suggests that customers who spend more are more influential in determining the target variable, making this a crucial factor in our analysis.\n",
    "\n",
    "2. **Avg_Order_Value**: This feature also contributes to the model, though to a much lesser extent than Monetary. This suggests that while the average value of orders has some impact, it is not as decisive as the overall spend.\n",
    "\n",
    "3. **Recency, Frequency, and Customer_Lifetime_Duration**: These features appear to have negligible importance in the model. This indicates that the time since the last purchase, the frequency of purchases, and the duration of the customer relationship are not significant predictors in this context.\n",
    "---\n",
    "### **Recommendations:**\n",
    "- **Focus on Monetary Metrics**: Given the strong influence of the Monetary feature, efforts to increase customer spend (e.g., through upselling or targeted promotions) may be the most effective strategy.\n",
    "- **Reevaluate Other Metrics**: Since Recency, Frequency, and Customer_Lifetime_Duration contribute little to the model, it may be worth exploring whether these metrics can be optimized further or if other variables could better capture customer behavior.\n",
    "\n",
    "This analysis allows us to prioritize strategies that align with the most influential factors, maximizing our impact on the target outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85953296-4f7f-4264-8620-ed46309a08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a file\n",
    "joblib.dump(model, \"clv_prediction_model.pk1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5118e5c2-5692-4f9c-8de2-63508c4e57d8",
   "metadata": {},
   "source": [
    "## **Conceptual  Overview Sales Forecasting**\n",
    "\n",
    "Sales forecasting is the process of predicting future sales revenue based on historical data, market trends, and other relevant factors. It is crucial in business planning, enabling companies to make informed decisions regarding inventory management, budgeting, and resource allocation.\n",
    "\n",
    "### Key Steps in Sales Forecasting\n",
    "\n",
    "1. **Data Collection**: Gather historical sales data.\n",
    "2. **Model Selection**: Choose a forecasting method, like time series analysis or regression models.\n",
    "3. **Analysis**: Identify patterns and trends in the data.\n",
    "4. **Prediction**: Estimate future sales.\n",
    "\n",
    "Accurate forecasts help optimize operations, reduce costs, and enhance profitability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39803589-f31c-43bf-a018-8ae92faab649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b163a-0a2e-4c6c-a0df-7f8d16da194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Order Date' to a datetime object\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "\n",
    "# Aggregate the data by month\n",
    "df = df.set_index('Order Date')\n",
    "monthly_sales = df['Sales'].resample('M').sum().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2d357-45be-4f99-b70b-8a6bc7c83339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original time series\n",
    "monthly_sales.plot(figsize = (12, 6), marker='o')\n",
    "plt.title(\"Monthly Sales\")\n",
    "# Customizing the xticks to show abbreviated month names with rotation\n",
    "plt.xticks(monthly_sales.index, monthly_sales.index.strftime(\"%b\"), rotation = 60)\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374fe5b-9715-4f0a-bb1f-c102729a2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Make the Data Stationary with Diferencing\n",
    "monthly_sales[\"Sales_diff\"] = monthly_sales[\"Sales\"].diff()\n",
    "monthly_sales = monthly_sales.dropna()\n",
    "\n",
    "# Plot the differenced data\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(monthly_sales.index, monthly_sales[\"Sales_diff\"],color='orange', marker='o')\n",
    "plt.title(\"Monthly Customer Sales Difference\")\n",
    "# Customizing the xticks to show abbreviated month names with rotation\n",
    "plt.xticks(monthly_sales.index, monthly_sales.index.strftime(\"%b\"), rotation = 60)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Sales Difference\")\n",
    "plt.legend([\"Sales Difference\"],loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056e8fc-0d12-4587-983b-3f586893cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare Data for Supervised Learning\n",
    "# Dropping \"Order Date\" and \"Sales\" columns\n",
    "supervised_data = monthly_sales.drop([\"Sales\"], axis = 1)\n",
    "\n",
    "# Create lagged features \n",
    "for i in range(1, 13):\n",
    "    col_name = \"month_\" + str(i)\n",
    "    supervised_data[col_name] = supervised_data[\"Sales_diff\"].shift(1)\n",
    "supervised_data = supervised_data.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7f308-3265-488b-bee8-e27ddd3de871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the Data into Training and Testing Sets\n",
    "train_data = supervised_data[:-12]\n",
    "test_data = supervised_data[-12:]\n",
    "\n",
    "x_train, y_train = train_data.iloc[:, 1:], train_data.iloc[:,0]\n",
    "x_test, y_test = test_data.iloc[:,1:], test_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eaf55b-b6de-458d-947a-18b2786a8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Apply Scalingb to the Features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08cf42-a90f-43d1-995e-9364cdecb637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Identify Optimal SARIMA Parameters (No scaling needed for SARIMA)\n",
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "\n",
    "best_aic = float(\"inf\")\n",
    "best_param = None\n",
    "best_seasonal_param = None\n",
    "\n",
    "\n",
    "for param in pdq:\n",
    "    for seasonal_param in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMA(monthly_sales[\"Sales\"],\n",
    "                                          order = param,\n",
    "                                          seasonal_order = seasonal_para,\n",
    "                                          enforce_stationarity = False,\n",
    "                                          enforce_invertibility = False)\n",
    "            results = mod.fit()\n",
    "            if results.aic < best_aic:\n",
    "                best_aic = results.aic\n",
    "                best_param = param\n",
    "                best_seasonal_param = seasonal_param\n",
    "        except:\n",
    "            continue\n",
    "print(f\"Best SARIMA Model: ARIMA{best_param}x{best_seasonal_param} - AIC:{best_aic}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53547955-a1e8-48f7-9f90-8c06fab16534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Fit thr SARIMA Model\n",
    "model = sm.tsa.statespace.SARIMAX(monthly_sales[\"Sales\"],\n",
    "                                 order=best_param,\n",
    "                                  seasonal_order=best_seasonal_param,\n",
    "                                  enforce_stationarity=False,\n",
    "                                  enforce_invertibility=False)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080965ad-2aa7-477d-b93c-2439ab8e6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Make Forecast with SARIMA\n",
    "forecast = results.get_forecast(steps = 12)\n",
    "forecast_ci = forecast.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5195d-64e1-4f39-813e-53889c598e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the SARIMA forecast\n",
    "ax = monthly_sales[\"Sales\"].plot(label = \"Observed\", figsize = (12,6))\n",
    "forecast.predicted_mean.plot(ax = ax, label = \"SARIMA Forecast\")\n",
    "ax.fill_between(forecast_ci.index,\n",
    "               forecast_ci.iloc[:, 0],\n",
    "               forecast_ci.iloc[:, 1], color = \"k\", alpha = .25)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Sales\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549026f8-b73f-4281-9e54-420f80e8b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate SARIMA Model\n",
    "y_true = monthly_sales[\"Sales\"][-12:]\n",
    "y_pred = forecast.predicted_mean\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(f'SARIMA MAE: {mae}')\n",
    "print(f'SARIMA RMSE: {rmse}')\n",
    "print(f'SARIMA R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125c1d1d-c7eb-486d-9bf1-e469c37941f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Exponential Smoothing (ETS) Model\n",
    "hw_model = ExponentialSmoothing(monthly_sales[\"Sales\"],\n",
    "                               seasonal = \"add\",\n",
    "                               seasonal_periods = 12).fit()\n",
    "hw_forecast = hw_model.forecast(steps = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46857f0e-538a-4ee9-b7e2-271359d285ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ETS Forecast\n",
    "ax = monthly_sales[\"Sales\"].plot(label='Observed', figsize=(12, 6))\n",
    "hw_forecast.plot(ax=ax, label='ETS Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39cce3e-c966-4d56-9841-1358f976ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ETS Model\n",
    "mae_hw = mean_absolute_error(y_true, hw_forecast)\n",
    "rmse_hw = mean_squared_error(y_true, hw_forecast, squared=False)\n",
    "r2_hw = r2_score(y_true, hw_forecast)\n",
    "print(f'ETS MAE: {mae_hw}')\n",
    "print(f'ETS RMSE: {rmse_hw}')\n",
    "print(f'ETS R-squared: {r2_hw}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60adf88-edcd-4df1-82bf-6f9f56672faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet Model\n",
    "df_prophet = monthly_sales[\"Sales\"].reset_index()\n",
    "df_prophet.columns = [\"ds\", \"y\"]\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(df_prophet)\n",
    "\n",
    "future = prophet_model.make_future_dataframe(periods = 12, freq = \"M\")\n",
    "forecast_prophet = prophet_model.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232540e7-55d1-4068-aea4-6c2da784e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecast using Prophet's plot function\n",
    "fig = prophet_model.plot(forecast_prophet)\n",
    "\n",
    "# Add xlabel and ylabel\n",
    "fig.gca().set_xlabel(\"Date\")\n",
    "fig.gca().set_ylabel(\"Sales Forecast\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5e2f9-9e66-4a72-b676-2f4e30c749a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prophet = forecast_prophet['yhat'][-12:]\n",
    "mae_prophet = mean_absolute_error(y_true, y_pred_prophet)\n",
    "rmse_prophet = mean_squared_error(y_true, y_pred_prophet, squared=False)\n",
    "r2_prophet = r2_score(y_true, y_pred_prophet)\n",
    "print(f'Prophet MAE: {mae_prophet}')\n",
    "print(f'Prophet RMSE: {rmse_prophet}')\n",
    "print(f'Prophet R-squared: {r2_prophet}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918cf162-cd85-400f-b571-d499ad59a9d9",
   "metadata": {},
   "source": [
    "### **Forecast Summary and Model Selection**\n",
    "\n",
    "**Model Performance Summary:**\n",
    "\n",
    "1. **SARIMA Model:**\n",
    "   - **MAE:** 55,052.68\n",
    "   - **RMSE:** 59,769.34\n",
    "   - **R-squared:** -5.5962\n",
    "   - **Interpretation:** The SARIMA model shows high error values (MAE and RMSE), indicating poor accuracy in predicting sales. The negative R-squared value suggests that the model performs worse than a simple mean prediction, indicating it's not well-suited for this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Exponential Smoothing (ETS) Model:**\n",
    "   - **MAE:** 9,105.41\n",
    "   - **RMSE:** 10,914.68\n",
    "   - **R-squared:** 0.7800\n",
    "   - **Interpretation:** The ETS model performs significantly better than SARIMA, with much lower error values. The positive R-squared value indicates that the model explains approximately 78% of the variance in the data, making it a good fit for the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Prophet Model:**\n",
    "   - **MAE:** 9,103.74\n",
    "   - **RMSE:** 10,600.73\n",
    "   - **R-squared:** 0.7925\n",
    "   - **Interpretation:** The Prophet model has the lowest MAE and RMSE among the models, indicating the highest accuracy. The R-squared value of 0.7925 suggests that this model explains about 79.25% of the variance in the data, making it the best-performing model.\n",
    "\n",
    "---\n",
    "\n",
    "**Selection of the Best Model:**\n",
    "\n",
    "- **Prophet** is the best-performing model with the lowest MAE (9,103.74) and RMSE (10,600.73), as well as the highest R-squared value (0.7925). This indicates that Prophet provides the most accurate forecast among the models tested.\n",
    "\n",
    "---\n",
    "\n",
    "**Forecast Analysis Conclusion:**\n",
    "\n",
    "- The **Prophet model** is selected as the final model for sales forecasting due to its superior performance. It closely matches the actual sales data, offering a strong fit to the historical data and accurate future predictions.\n",
    "- Businesses can rely on the Prophet model for making informed decisions about inventory management, marketing strategies, and financial planning, as it provides the most accurate and reliable sales forecasts.\n",
    "- By using the Prophet model, the forecasting process will capture complex patterns in the sales data, leading to actionable insights and better strategic planning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64d2b1-6c61-4598-90ea-44eda82ac4e3",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b927d5-6187-43dc-a222-f0d6d5f6b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('example_dataset.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873acadf-6250-473b-9b73-70f5fc5c480f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
